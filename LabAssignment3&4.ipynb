{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1AUYMPTIXkb36X6PJRTmDTsEzStz98Pz-",
      "authorship_tag": "ABX9TyOmlZH/92Jvj2mF290p7EXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gazalapar/Data-Analytics-Lab/blob/main/LabAssignment3%264.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analytics  Lab Assignment 3**"
      ],
      "metadata": {
        "id": "t0V2t4AJagQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kn0JYfgDeUjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**using cosine simlarity find the simlarity between two** **documents**"
      ],
      "metadata": {
        "id": "2hc-dcrVPtuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the path to the directory where your files are located\n",
        "directory = '/content/drive/MyDrive/Colab Notebooks/TextDocs'  # Update if needed\n",
        "\n",
        "# Initialize a list to store document contents\n",
        "documents = []\n",
        "\n",
        "# Read all text files from the directory\n",
        "for i in range(1, 6):\n",
        "    file_path = os.path.join(directory, f\"Doc{i}.txt\")  # Update to match filenames\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            documents.append(content)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Proceed with text processing and similarity computation if documents are read successfully\n",
        "if documents:\n",
        "    # Preprocessing: Tokenization, removing punctuation, lowercasing\n",
        "    def preprocess(doc):\n",
        "        doc = re.sub(r'\\W+', ' ', doc).lower()\n",
        "        return doc\n",
        "\n",
        "    processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "    # Vectorize the documents using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
        "\n",
        "    # Compute the cosine similarity matrix\n",
        "    cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    # Create the upper triangular matrix for cosine similarities (excluding the diagonal)\n",
        "    upper_triangular_matrix = np.triu(cosine_sim_matrix, k=1)\n",
        "\n",
        "    print(\"Cosine Similarity Upper Triangular Matrix:\")\n",
        "    print(upper_triangular_matrix)\n",
        "\n",
        "    # Find the maximum similarity and its indices\n",
        "    max_similarity = np.max(upper_triangular_matrix)\n",
        "    max_index = np.unravel_index(np.argmax(upper_triangular_matrix), upper_triangular_matrix.shape)\n",
        "\n",
        "    doc_pair = (max_index[0] + 1, max_index[1] + 1)\n",
        "\n",
        "    print(f\"\\nThe most similar pair of documents are Document {doc_pair[0]} and Document {doc_pair[1]} with a cosine similarity of {max_similarity:.4f}.\")\n"
      ],
      "metadata": {
        "id": "9SWAP-ilgnwm",
        "outputId": "5d0d907d-c203-40b1-f534-fbdf25951a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Upper Triangular Matrix:\n",
            "[[0.         0.02122381 0.04275781 0.0461145  0.04792865]\n",
            " [0.         0.         0.01931133 0.         0.01820505]\n",
            " [0.         0.         0.         0.         0.11002849]\n",
            " [0.         0.         0.         0.         0.06440101]\n",
            " [0.         0.         0.         0.         0.        ]]\n",
            "\n",
            "The most similar pair of documents are Document 3 and Document 5 with a cosine similarity of 0.1100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analytics Lab Assignment 4**"
      ],
      "metadata": {
        "id": "myuCfbnqQRST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apriori Algorithm Implementation**"
      ],
      "metadata": {
        "id": "qPxeKuW6RB60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Function to calculate support for itemsets\n",
        "def calculate_support(transactions, itemsets):\n",
        "    support_count = {}\n",
        "    for itemset in itemsets:\n",
        "        for transaction in transactions:\n",
        "            if set(itemset).issubset(set(transaction)):\n",
        "                if itemset in support_count:\n",
        "                    support_count[itemset] += 1\n",
        "                else:\n",
        "                    support_count[itemset] = 1\n",
        "    return support_count\n",
        "\n",
        "# Function to generate candidate itemsets of size k\n",
        "def generate_new_combinations(frequent_itemsets, k):\n",
        "    new_combinations = []\n",
        "    frequent_itemsets_list = list(frequent_itemsets.keys())\n",
        "    for i in range(len(frequent_itemsets_list)):\n",
        "        for j in range(i+1, len(frequent_itemsets_list)):\n",
        "            new_combination = tuple(sorted(set(frequent_itemsets_list[i]) | set(frequent_itemsets_list[j])))\n",
        "            if len(new_combination) == k and new_combination not in new_combinations:\n",
        "                new_combinations.append(new_combination)\n",
        "    return new_combinations\n",
        "\n",
        "# Apriori algorithm implementation\n",
        "def apriori(transactions, min_support):\n",
        "    # Step 1: Generate initial candidate itemsets (of size 1)\n",
        "    single_items = set(item for transaction in transactions for item in transaction)\n",
        "    itemsets = [(item,) for item in single_items]\n",
        "\n",
        "    # Step 2: Calculate support and prune non-frequent itemsets\n",
        "    support_count = calculate_support(transactions, itemsets)\n",
        "    frequent_itemsets = {k: v for k, v in support_count.items() if v >= min_support}\n",
        "\n",
        "    all_frequent_itemsets = {}\n",
        "    k = 2\n",
        "\n",
        "    while frequent_itemsets:\n",
        "        all_frequent_itemsets.update(frequent_itemsets)\n",
        "\n",
        "        # Step 3: Generate candidate itemsets of size k\n",
        "        itemsets = generate_new_combinations(frequent_itemsets, k)\n",
        "\n",
        "        # Step 4: Calculate support and prune non-frequent itemsets\n",
        "        support_count = calculate_support(transactions, itemsets)\n",
        "        frequent_itemsets = {k: v for k, v in support_count.items() if v >= min_support}\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent_itemsets\n",
        "\n",
        "# Example usage\n",
        "transactions = [\n",
        "    ['Bread', 'Milk'],\n",
        "    ['Bread', 'Diaper', 'Beer', 'Eggs'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Cola'],\n",
        "    ['Bread', 'Milk', 'Diaper', 'Beer'],\n",
        "    ['Bread', 'Milk', 'Cola']\n",
        "]\n",
        "\n",
        "min_support = 2\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "print(\"Frequent Itemsets:\")\n",
        "for itemset, support in frequent_itemsets.items():\n",
        "    print(f\"{itemset}: {support}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA4n7SYnRPUA",
        "outputId": "56ff283a-1b2a-4145-be9f-dcdf9cf34f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "('Beer',): 3\n",
            "('Cola',): 2\n",
            "('Milk',): 4\n",
            "('Diaper',): 3\n",
            "('Bread',): 4\n",
            "('Beer', 'Milk'): 2\n",
            "('Beer', 'Diaper'): 3\n",
            "('Beer', 'Bread'): 2\n",
            "('Cola', 'Milk'): 2\n",
            "('Diaper', 'Milk'): 2\n",
            "('Bread', 'Milk'): 3\n",
            "('Bread', 'Diaper'): 2\n",
            "('Beer', 'Diaper', 'Milk'): 2\n",
            "('Beer', 'Bread', 'Diaper'): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori(transactions, min_support, max_support):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Step 1: Count frequency of individual items (1-itemsets)\n",
        "    def get_frequent_itemsets(itemsets, transactions, min_support, max_support):\n",
        "        itemset_count = defaultdict(int)\n",
        "        for transaction in transactions:\n",
        "            for itemset in itemsets:\n",
        "                if set(itemset).issubset(transaction):\n",
        "                    itemset_count[itemset] += 1\n",
        "\n",
        "        # Prune itemsets based on min_support and max_support\n",
        "        frequent_itemsets = {}\n",
        "        for itemset, count in itemset_count.items():\n",
        "            if min_support <= count <= max_support:  # Properly filter by min and max support\n",
        "                frequent_itemsets[itemset] = count\n",
        "\n",
        "        return frequent_itemsets\n",
        "\n",
        "    # Step 2: Generate candidate k-itemsets\n",
        "    def generate_candidates(itemsets, k):\n",
        "        candidates = set()\n",
        "        length = len(itemsets)\n",
        "        itemsets = list(itemsets)\n",
        "\n",
        "        for i in range(length):\n",
        "            for j in range(i+1, length):\n",
        "                candidate = tuple(sorted(set(itemsets[i]).union(itemsets[j])))\n",
        "                if len(candidate) == k:\n",
        "                    candidates.add(candidate)\n",
        "        return candidates\n",
        "\n",
        "    # Initial step: Start with 1-itemsets\n",
        "    itemsets = set()\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            itemsets.add((item,))\n",
        "\n",
        "    # List to store all frequent itemsets with their frequencies\n",
        "    all_frequent_itemsets = {}\n",
        "\n",
        "    k = 1\n",
        "    while itemsets:\n",
        "        # Get frequent itemsets for the current size k\n",
        "        frequent_itemsets = get_frequent_itemsets(itemsets, transactions, min_support, max_support)\n",
        "\n",
        "        if not frequent_itemsets:\n",
        "            break\n",
        "\n",
        "        all_frequent_itemsets.update(frequent_itemsets)\n",
        "\n",
        "        # Generate candidates for the next size k+1\n",
        "        itemsets = generate_candidates(frequent_itemsets, k + 1)\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent_itemsets\n",
        "\n",
        "'''# Example usage\n",
        "transactions = [\n",
        "    ['milk', 'bread', 'butter'],\n",
        "    ['beer', 'bread'],\n",
        "    ['milk', 'bread'],\n",
        "    ['butter', 'bread'],\n",
        "    ['milk', 'butter']\n",
        "]\n",
        "'''\n",
        "\n",
        "transactions = [\n",
        "    ['Bread', 'Milk'],\n",
        "    ['Bread', 'Diaper', 'Beer', 'Eggs'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Cola'],\n",
        "    ['Bread', 'Milk', 'Diaper', 'Beer'],\n",
        "    ['Bread', 'Milk', 'Cola']\n",
        "]\n",
        "min_support = 2\n",
        "max_support = 3\n",
        "\n",
        "frequent_itemsets = apriori(transactions, min_support, max_support)\n",
        "\n",
        "\n",
        "for itemset, frequency in frequent_itemsets.items():\n",
        "    print(f\"Itemset: {itemset}, Frequency: {frequency}\")\n"
      ],
      "metadata": {
        "id": "IQzCWxDyfH0h",
        "outputId": "144863aa-2ae3-4c2a-d807-c7f32e28d29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemset: ('Beer',), Frequency: 3\n",
            "Itemset: ('Diaper',), Frequency: 3\n",
            "Itemset: ('Cola',), Frequency: 2\n",
            "Itemset: ('Beer', 'Diaper'), Frequency: 3\n"
          ]
        }
      ]
    }
  ]
}